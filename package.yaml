name: llama-cpp-hs
version: 0.1.0.0
github: "tusharad/llama-cpp-hs"
license: MIT
author: "tushar"
maintainer: "tusharadhatrao@gmail.com"
copyright: "2025 tushar"
category: ai, ffi, natural-language-processing
extra-source-files:
  - README.md
  - CHANGELOG.md

description:
  Haskell bindings for [llama.cpp](https://github.com/ggerganov/llama.cpp),
  a performant, C++-based inference engine for running large language models
  (LLMs) like LLaMA, Mistral, Qwen, and others directly on local hardware.
synopsis: Haskell FFI bindings to the llama.cpp LLM inference library

dependencies:
  - base >= 4.7 && < 5
  - bytestring >= 0.9 && < 0.13
  - derive-storable >= 0.2 && < 0.4

ghc-options:
  - -Wall
  - -Wcompat
  - -Widentities
  - -Wincomplete-record-updates
  - -Wincomplete-uni-patterns
    # - -Wmissing-export-lists
  - -Wmissing-home-modules
  - -Wpartial-fields
  - -Wredundant-constraints

library:
  source-dirs: src

  include-dirs:
    - cbits
  c-sources:
    - cbits/helper.c
  extra-lib-dirs:
    - /usr/local/lib
  extra-libraries:
    - llama

tests:
  llama-cpp-hs-test:
    main: Spec.hs
    source-dirs: test
    ghc-options:
      - -threaded
      - -rtsopts
      - -with-rtsopts=-N
    dependencies:
      - tasty
      - tasty-hunit
      - llama-cpp-hs
